> Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ **Ğ²ÑĞµÑ…** Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ñ… Ğ² [[ğŸš¨ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ”Ñ‹Ñ€Ñ‹ Ğ¸ ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞŸĞ°Ñ€ÑĞµÑ€Ğ°]].

> ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ° Ğ² [[Ğ¡Ñ…ĞµĞ¼Ğ°]].

  

---

  

## ğŸ“ Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ’Ñ‹Ğ²Ğ¾Ğ´: Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ ĞĞ½Ğ°Ğ»Ğ¸Ğ·

  

ĞŸÑ€ĞµĞ¶Ğ´Ğµ Ñ‡ĞµĞ¼ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğº Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼, Ğ·Ğ°Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµĞ¼ **ĞºĞ¾Ñ€Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹** Ğ²ÑĞµÑ… 14 Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼. ĞĞ½Ğ¸ ÑĞ²Ğ¾Ğ´ÑÑ‚ÑÑ Ğº **Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ğ¼ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:**

  

### A. ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ğ¼

ĞŸĞ°Ñ€ÑĞµÑ€ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… API (GitHub, ipapi.co, httpbin.org) Ğ¸ **Ğ½Ğµ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹**: Ğ½ĞµÑ‚ retry, Ğ½ĞµÑ‚ fallback, Ğ½ĞµÑ‚ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¾Ğ². Ğ›ÑĞ±Ğ¾Ğ¹ ÑĞ±Ğ¾Ğ¹ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ° â€” Ğ¸ Ñ†Ğ¸ĞºĞ» Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ "Ğ² Ğ¿ÑƒÑÑ‚Ğ¾Ñ‚Ñƒ".

  

### B. Ğ›Ğ¾Ğ¶Ğ½Ğ°Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸

Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ°ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ ÑˆĞµÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ² (`http`, `https`, `socks4`, `socks5`, `mtproto`, `ss`), Ğ½Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ **Ğ´Ğ²Ğ°** Ğ¸Ğ· Ğ½Ğ¸Ñ…. Ğ­Ñ‚Ğ¾ Ğ½Ğµ Ğ±Ğ°Ğ³ â€” ÑÑ‚Ğ¾ **Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ¼Ğ°Ğ½**, Ğ¸Ğ·-Ğ·Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ 60-80% ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµÑĞ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹.

  

### C. ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ (Observability)

ĞĞ¸ Ğ»Ğ¾Ğ³Ğ¾Ğ², Ğ½Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ½Ğ¸ Ğ°Ğ»ĞµÑ€Ñ‚Ğ¾Ğ². Ğ•Ğ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ÑƒĞ·Ğ½Ğ°Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ°Ñ€ÑĞµÑ€ ÑĞ»Ğ¾Ğ¼Ğ°Ğ½ â€” Ğ·Ğ°Ğ¹Ñ‚Ğ¸ Ğ² Telegram-Ğ±Ğ¾Ñ‚Ğ° Ğ¸ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ½Ğµ Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ. Ğ­Ñ‚Ğ¾ Ğ½ĞµĞ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ğ¾ Ğ´Ğ»Ñ ÑĞµÑ€Ğ²Ğ¸ÑĞ°, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ĞµĞ³Ğ¾ 24/7.

  

### D. Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ (Ğ° Ğ½Ğµ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ°Ñ) Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

Ğ’ÑĞµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ…Ğ¾Ñ‚Ñ Ğ¿Ğ¾ ÑĞ²Ğ¾ĞµĞ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ 15-ÑĞµĞºÑƒĞ½Ğ´Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ² 5-Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½ÑƒÑ.

  

---

  

## ğŸ—“ ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ

  

Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ¿Ğ¾ **Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ñƒ Ğ²Ğ¾Ğ·Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ**: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ÑÑ‚ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ¸Ğ»Ğ¸ÑÑ….

  

```mermaid

graph TD

Â  Â  A["Ğ¤Ğ°Ğ·Ğ° 1: Ğ¡Ğ¿Ğ°ÑĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"] --> B["Ğ¤Ğ°Ğ·Ğ° 2: Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ"]

Â  Â  B --> C["Ğ¤Ğ°Ğ·Ğ° 3: ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ"]

Â  Â  C --> D["Ğ¤Ğ°Ğ·Ğ° 4: ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ´Ğ°"]

Â  Â  D --> E["Ğ¤Ğ°Ğ·Ğ° 5: ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°"]

  

Â  Â  A --- A1["#2 Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ SOCKS/MTProto"]

Â  Â  A --- A2["#8 Normalizer ÑƒĞ¼ĞµĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ²"]

  

Â  Â  B --- B1["#1 GitHub Rate Limit"]

Â  Â  B --- B2["#14 Fallback endpoint-Ñ‹"]

Â  Â  B --- B3["#7 Retry-Ğ»Ğ¾Ğ³Ğ¸ĞºĞ°"]

  

Â  Â  C --- C1["#3 ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€"]

Â  Â  C --- C2["#4 Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ GeoIP"]

Â  Â  C --- C3["#5 Batch-Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ² Ğ‘Ğ”"]

  

Â  Â  D --- D1["#6 Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ"]

Â  Â  D --- D2["#10 datetime fix"]

Â  Â  D --- D3["#12 Ğ¢ĞµÑÑ‚Ñ‹"]

  

Â  Â  E --- E1["#9 TTL Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ²"]

Â  Â  E --- E2["#13 ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¼Ñ‘Ñ€Ñ‚Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞºÑĞ¸"]

Â  Â  E --- E3["#11 Ğ¡Ğ²ÑĞ·ÑŒ Ğ±Ğ¾Ñ‚Ğ° Ğ¸ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°"]

```

  

---

  

## Ğ¤Ğ°Ğ·Ğ° 1: Ğ¡Ğ¿Ğ°ÑĞµĞ½Ğ¸Ğµ Ğ”Ğ°Ğ½Ğ½Ñ‹Ñ…

  

> Ğ¦ĞµĞ»ÑŒ: ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ°Ñ€ÑĞµÑ€ **ÑƒĞ¶Ğµ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚**, Ğ½Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ²Ğ¿ÑƒÑÑ‚ÑƒÑ.

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #2 â€” Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ SOCKS/MTProto/Shadowsocks

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#2. Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ HTTP â€” socks5/mtproto/ss ĞĞµĞ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹]]

  

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:**

- `requirements.txt` â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸

- `app/validator.py` â€” Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸

  

#### Ğ¨Ğ°Ğ³ 1: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸

  

```txt

# requirements.txt â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ:

httpx-socks[asyncio]==0.9.1

```

  

`httpx-socks` â€” ÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ `httpx`, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ SOCKS4/SOCKS5-Ğ¿Ñ€Ğ¾ĞºÑĞ¸. Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº drop-in transport.

  

#### Ğ¨Ğ°Ğ³ 2: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ°

  

Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ´:

```python

# app/validator.py â€” Ğ‘Ğ«Ğ›Ğ:

proxy_url = f"{candidate.proxy_type}://{candidate.host}:{candidate.port}"

async with httpx.AsyncClient(proxy=proxy_url, timeout=timeout_sec) as client:

Â  Â  r = await client.get("https://httpbin.org/ip")

```

  

ĞÑƒĞ¶Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ:

  

```python

# app/validator.py â€” Ğ¡Ğ¢ĞĞ›Ğ:

import asyncio

import time

from dataclasses import dataclass

import httpx

from httpx_socks import AsyncProxyTransport

from app.normalizer import ProxyCandidate

  

CHECK_URLS = [

Â  Â  "https://httpbin.org/ip",

Â  Â  "https://ifconfig.me/ip",

Â  Â  "https://api.ipify.org",

]

  

@dataclass(slots=True)

class ValidationResult:

Â  Â  candidate: ProxyCandidate

Â  Â  is_alive: bool

Â  Â  latency_ms: float | None

  
  

async def _check_http(candidate: ProxyCandidate, timeout_sec: float) -> ValidationResult:

Â  Â  """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° HTTP/HTTPS Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ httpx."""

Â  Â  proxy_url = f"{candidate.proxy_type}://{candidate.host}:{candidate.port}"

Â  Â  for url in CHECK_URLS:

Â  Â  Â  Â  t0 = time.perf_counter()

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  async with httpx.AsyncClient(proxy=proxy_url, timeout=timeout_sec, follow_redirects=True) as client:

Â  Â  Â  Â  Â  Â  Â  Â  r = await client.get(url)

Â  Â  Â  Â  Â  Â  Â  Â  if r.status_code < 500:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dt = (time.perf_counter() - t0) * 1000

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return ValidationResult(candidate=candidate, is_alive=True, latency_ms=dt)

Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  continue

Â  Â  return ValidationResult(candidate=candidate, is_alive=False, latency_ms=None)

  
  

async def _check_socks(candidate: ProxyCandidate, timeout_sec: float) -> ValidationResult:

Â  Â  """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° SOCKS4/SOCKS5 Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ñ‡ĞµÑ€ĞµĞ· httpx-socks."""

Â  Â  proxy_url = f"{candidate.proxy_type}://{candidate.host}:{candidate.port}"

Â  Â  transport = AsyncProxyTransport.from_url(proxy_url)

Â  Â  for url in CHECK_URLS:

Â  Â  Â  Â  t0 = time.perf_counter()

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  async with httpx.AsyncClient(transport=transport, timeout=timeout_sec) as client:

Â  Â  Â  Â  Â  Â  Â  Â  r = await client.get(url)

Â  Â  Â  Â  Â  Â  Â  Â  if r.status_code < 500:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dt = (time.perf_counter() - t0) * 1000

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return ValidationResult(candidate=candidate, is_alive=True, latency_ms=dt)

Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  continue

Â  Â  return ValidationResult(candidate=candidate, is_alive=False, latency_ms=None)

  
  

async def _check_tcp_only(candidate: ProxyCandidate, timeout_sec: float) -> ValidationResult:

Â  Â  """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° MTProto/Shadowsocks â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ TCP-connectivity."""

Â  Â  t0 = time.perf_counter()

Â  Â  try:

Â  Â  Â  Â  reader, writer = await asyncio.wait_for(

Â  Â  Â  Â  Â  Â  asyncio.open_connection(candidate.host, candidate.port),

Â  Â  Â  Â  Â  Â  timeout=timeout_sec,

Â  Â  Â  Â  )

Â  Â  Â  Â  dt = (time.perf_counter() - t0) * 1000

Â  Â  Â  Â  writer.close()

Â  Â  Â  Â  await writer.wait_closed()

Â  Â  Â  Â  return ValidationResult(candidate=candidate, is_alive=True, latency_ms=dt)

Â  Â  except Exception:

Â  Â  Â  Â  return ValidationResult(candidate=candidate, is_alive=False, latency_ms=None)

  
  

async def _check(candidate: ProxyCandidate, timeout_sec: float) -> ValidationResult:

Â  Â  """ĞœĞ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€: Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ°."""

Â  Â  if candidate.proxy_type in ("http", "https"):

Â  Â  Â  Â  return await _check_http(candidate, timeout_sec)

Â  Â  elif candidate.proxy_type in ("socks4", "socks5"):

Â  Â  Â  Â  return await _check_socks(candidate, timeout_sec)

Â  Â  elif candidate.proxy_type in ("mtproto", "ss"):

Â  Â  Â  Â  return await _check_tcp_only(candidate, timeout_sec)

Â  Â  else:

Â  Â  Â  Â  return await _check_http(candidate, timeout_sec) Â # fallback

  
  

async def validate_many(candidates: list[ProxyCandidate], timeout_sec: float, max_concurrent: int) -> list[ValidationResult]:

Â  Â  sem = asyncio.Semaphore(max_concurrent)

Â  Â  async def run_one(c: ProxyCandidate) -> ValidationResult:

Â  Â  Â  Â  async with sem:

Â  Â  Â  Â  Â  Â  return await _check(c, timeout_sec)

Â  Â  return await asyncio.gather(*(run_one(c) for c in candidates))

```

  

**ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ñ‚Ğ°Ğº:** Ğ¢Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ğ¾ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²ÑĞµ 6 Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ². Ğ”Ğ»Ñ MTProto Ğ¸ Shadowsocks Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ° Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº, Ğ½Ğ¾ TCP-connectivity â€” ÑÑ‚Ğ¾ ÑƒĞ¶Ğµ **Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ**, Ñ‡ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ "Ğ²ÑĞµĞ³Ğ´Ğ° False".

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #8 â€” Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Normalizer

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#8. Normalizer ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹]]

  

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:**

- `app/normalizer.py` â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹

  

#### Ğ¨Ğ°Ğ³ 1: Ğ Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ RegEx Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸

  

```python

# app/normalizer.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹:

  

# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ RegEx â€” Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ»Ğ¾Ğ²Ğ¸Ñ‚ user:pass@host:port

PROXY_RE = re.compile(

Â  Â  r"(?P<scheme>socks5|socks4|http|https|mtproto|ss)://"

Â  Â  r"(?:[\w.~%-]+:[\w.~%-]+@)?" Â # Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ user:pass@

Â  Â  r"(?P<host>[a-zA-Z0-9_.-]+):(?P<port>\d{2,5})",

Â  Â  re.IGNORECASE,

)

  

# Shadowsocks URI Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚: ss://base64...

SS_URI_RE = re.compile(r"ss://([A-Za-z0-9+/=]+)(?:#\S+)?")

  

# JSON-Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚: {"server": "1.2.3.4", "server_port": 1080}

JSON_SERVER_RE = re.compile(

Â  Â  r'"server"\s*:\s*"(?P<host>[^"]+)"\s*,\s*'

Â  Â  r'"server_port"\s*:\s*(?P<port>\d{2,5})',

)

# VMess URI Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚: vmess://base64...

VMESS_URI_RE = re.compile(r"vmess://([A-Za-z0-9+/=]+)")

# Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»/Ñ‚Ğ°Ğ±ÑƒĞ»ÑÑ†Ğ¸Ñ: host port Ğ¸Ğ»Ğ¸ host\tport

SPACE_RE = re.compile(

    r"^(?P<host>\d{1,3}(?:\.\d{1,3}){3})[\s\t]+(?P<port>\d{2,5})$",

    re.MULTILINE,

)

```

  

#### Ğ¨Ğ°Ğ³ 2: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ² Ğ² `parse_candidates()`

  

```python

import base64

import json

  

def parse_candidates(text: str, source: str, default_scheme: str = "http") -> list[ProxyCandidate]:

Â  Â  out: list[ProxyCandidate] = []

Â  Â  seen = set()

  

Â  Â  # 1) Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ protocol://host:port

Â  Â  for m in PROXY_RE.finditer(text):

Â  Â  Â  Â  proxy_type = m.group("scheme").lower()

Â  Â  Â  Â  host = m.group("host")

Â  Â  Â  Â  port = int(m.group("port"))

Â  Â  Â  Â  key = (proxy_type, host, port)

Â  Â  Â  Â  if key not in seen:

Â  Â  Â  Â  Â  Â  seen.add(key)

Â  Â  Â  Â  Â  Â  out.append(ProxyCandidate(proxy_type=proxy_type, host=host, port=port, source=source))

  

Â  Â  # 2) Shadowsocks URI: ss://base64...

Â  Â  for m in SS_URI_RE.finditer(text):

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  decoded = base64.b64decode(m.group(1)).decode("utf-8", errors="ignore")

Â  Â  Â  Â  Â  Â  # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: method:password@host:port

Â  Â  Â  Â  Â  Â  if "@" in decoded:

Â  Â  Â  Â  Â  Â  Â  Â  addr_part = decoded.split("@", 1)[1]

Â  Â  Â  Â  Â  Â  Â  Â  host, port_str = addr_part.rsplit(":", 1)

Â  Â  Â  Â  Â  Â  Â  Â  port = int(port_str)

Â  Â  Â  Â  Â  Â  Â  Â  key = ("ss", host, port)

Â  Â  Â  Â  Â  Â  Â  Â  if key not in seen:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  seen.add(key)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  out.append(ProxyCandidate(proxy_type="ss", host=host, port=port, source=source))

Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  pass

  

Â  Â  # 3) JSON-Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ {"server": ..., "server_port": ...}

Â  Â  for m in JSON_SERVER_RE.finditer(text):

Â  Â  Â  Â  host = m.group("host")

Â  Â  Â  Â  port = int(m.group("port"))

Â  Â  Â  Â  key = ("ss", host, port)

Â  Â  Â  Â  if key not in seen:

Â  Â  Â  Â  Â  Â  seen.add(key)

Â  Â  Â  Â  Â  Â  out.append(ProxyCandidate(proxy_type="ss", host=host, port=port, source=source))

  

Â  Â  # 4) VMess URI: vmess://base64...

Â  Â  for m in VMESS_URI_RE.finditer(text):

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  decoded = base64.b64decode(m.group(1)).decode("utf-8", errors="ignore")

Â  Â  Â  Â  Â  Â  data = json.loads(decoded)

Â  Â  Â  Â  Â  Â  host = data.get("add", "")

Â  Â  Â  Â  Â  Â  port = int(data.get("port", 0))

Â  Â  Â  Â  Â  Â  if host and port:

Â  Â  Â  Â  Â  Â  Â  Â  key = ("vmess", host, port)

Â  Â  Â  Â  Â  Â  Â  Â  if key not in seen:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  seen.add(key)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  out.append(ProxyCandidate(proxy_type="vmess", host=host, port=port, source=source))

Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  pass

  

Â  Â  # 5) Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»/Ñ‚Ğ°Ğ±ÑƒĞ»ÑÑ†Ğ¸Ñ: 1.2.3.4 1080 Ğ¸Ğ»Ğ¸ 1.2.3.4\t1080

Â  Â  for m in SPACE_RE.finditer(text):

Â  Â  Â  Â  host = m.group("host")

Â  Â  Â  Â  port = int(m.group("port"))

Â  Â  Â  Â  key = (default_scheme.lower(), host, port)

Â  Â  Â  Â  if key not in seen:

Â  Â  Â  Â  Â  Â  seen.add(key)

Â  Â  Â  Â  Â  Â  out.append(ProxyCandidate(proxy_type=default_scheme.lower(), host=host, port=port, source=source))

  

Â  Â  # 6) Fallback: Ğ³Ğ¾Ğ»Ñ‹Ğ¹ ip:port

Â  Â  for m in FALLBACK_RE.finditer(text):

Â  Â  Â  Â  proxy_type = default_scheme.lower()

Â  Â  Â  Â  host = m.group("host")

Â  Â  Â  Â  port = int(m.group("port"))

Â  Â  Â  Â  key = (proxy_type, host, port)

Â  Â  Â  Â  if key not in seen:

Â  Â  Â  Â  Â  Â  seen.add(key)

Â  Â  Â  Â  Â  Â  out.append(ProxyCandidate(proxy_type=proxy_type, host=host, port=port, source=source))

  

Â  Â  return out

```

  

**ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ñ‚Ğ°Ğº:** Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ¿Ğ°Ñ€ÑĞµÑ€ Ğ»Ğ¾Ğ²Ğ¸Ñ‚ **6 Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ²** Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 2:

1. `protocol://host:port` (ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹, Ñ Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ `user:pass@`)

2. `ss://base64...` (Shadowsocks URI)

3. `{"server": ..., "server_port": ...}` (JSON-ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³)

4. `vmess://base64...` (V2Ray VMess URI)

5. `host port` / `host\tport` (Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»ÑŒĞ½Ğ¾-Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚)

6. `ip:port` (fallback)

ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾, Ğ½Ğµ Ğ»Ğ¾Ğ¼Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ, Ğ¸ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· `seen` Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´ÑƒĞ±Ğ»Ğ¸.

  

---

  

## Ğ¤Ğ°Ğ·Ğ° 2: Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¡Ğ±Ğ¾ÑĞ¼

  

> Ğ¦ĞµĞ»ÑŒ: Ğ¿Ğ°Ñ€ÑĞµÑ€ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ, Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ ÑĞ±Ğ¾ÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‚.

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #1 â€” ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° GitHub Rate Limit

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#1. GitHub API Rate Limit â€” ĞĞµÑ‚ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸]]

  

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:**

- `app/collectors/github.py` â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ rate-limit handling

  

#### Ğ¨Ğ°Ğ³ 1: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¾Ğ²

  

```python

# app/collectors/github.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ°:

import asyncio

import logging

  

logger = logging.getLogger(__name__)

  

async def _rate_limited_get(

Â  Â  client: httpx.AsyncClient,

Â  Â  url: str,

Â  Â  **kwargs,

) -> httpx.Response | None:

Â  Â  """GET-Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ GitHub Rate Limit.

  

Â  Â  ĞŸÑ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ 403 Ñ X-RateLimit-Remaining == 0:

Â  Â  - Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ

Â  Â  - Ğ–Ğ´Ñ‘Ñ‚ Ğ´Ğ¾ X-RateLimit-Reset

Â  Â  - ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ

Â  Â  ĞŸÑ€Ğ¸ 5xx Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… â€” retry Ğ´Ğ¾ 3 Ñ€Ğ°Ğ· Ñ exponential backoff.

Â  Â  """

Â  Â  max_retries = 3

Â  Â  for attempt in range(max_retries):

Â  Â  Â  Â  resp = await client.get(url, **kwargs)

  

Â  Â  Â  Â  if resp.status_code == 200:

Â  Â  Â  Â  Â  Â  return resp

  

Â  Â  Â  Â  if resp.status_code == 403:

Â  Â  Â  Â  Â  Â  remaining = int(resp.headers.get("X-RateLimit-Remaining", "1"))

Â  Â  Â  Â  Â  Â  if remaining == 0:

Â  Â  Â  Â  Â  Â  Â  Â  reset_ts = int(resp.headers.get("X-RateLimit-Reset", "0"))

Â  Â  Â  Â  Â  Â  Â  Â  import time as _time

Â  Â  Â  Â  Â  Â  Â  Â  wait = max(1, reset_ts - int(_time.time())) + 1

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"GitHub rate limit hit. Sleeping {wait}s until reset.")

Â  Â  Â  Â  Â  Â  Â  Â  await asyncio.sleep(min(wait, 300)) Â # ĞĞµ Ğ±Ğ¾Ğ»ĞµĞµ 5 Ğ¼Ğ¸Ğ½ÑƒÑ‚

Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"GitHub 403 (not rate limit): {url}")

Â  Â  Â  Â  Â  Â  Â  Â  return None

  

Â  Â  Â  Â  if resp.status_code >= 500:

Â  Â  Â  Â  Â  Â  wait = 2 ** attempt

Â  Â  Â  Â  Â  Â  logger.warning(f"GitHub 5xx ({resp.status_code}), retry in {wait}s")

Â  Â  Â  Â  Â  Â  await asyncio.sleep(wait)

Â  Â  Â  Â  Â  Â  continue

  

Â  Â  Â  Â  # 404, 422 Ğ¸ Ñ‚.Ğ´. â€” Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼

Â  Â  Â  Â  return None

  

Â  Â  return None

```

  

#### Ğ¨Ğ°Ğ³ 2: Ğ—Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ `client.get(...)` Ğ½Ğ° `_rate_limited_get(client, ...)`

  

Ğ’Ğ¾ Ğ²ÑĞµÑ… Ğ¼ĞµÑÑ‚Ğ°Ñ…, Ğ³Ğ´Ğµ ÑĞµĞ¹Ñ‡Ğ°Ñ:

```python

search = await client.get("https://api.github.com/search/code", params={...})

if search.status_code != 200:

Â  Â  break

```

Ğ—Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ°:

```python

search = await _rate_limited_get(client, "https://api.github.com/search/code", params={...})

if search is None:

Â  Â  break

```

  

ĞĞ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ: `search/repositories`, `repos/{repo}`, `repos/{repo}/readme`, `repos/{repo}/git/trees/...`, `repos/{repo}/git/blobs/...`.

  

> [!TIP]

> Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ `_rate_limited_get` ÑƒĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ retry Ñ exponential backoff Ğ´Ğ»Ñ 5xx Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº (Ğ´Ğ¾ 3 Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº), Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ #7 (Retry-Ğ»Ğ¾Ğ³Ğ¸ĞºĞ°) Ğ´Ğ»Ñ GitHub ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¾Ñ€Ğ°. Ğ”Ğ»Ñ ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ğ¾Ğ² Ğ¸ `ConnectionError` Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ² Ñ‚Ğ¾Ñ‚ Ğ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´:

  

```python

# app/collectors/github.py â€” Ğ”ĞĞŸĞĞ›ĞĞ˜Ğ¢Ğ¬ _rate_limited_get():
# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµÑ…Ğ²Ğ°Ñ‚ ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ†Ğ¸ĞºĞ» retry:

async def _rate_limited_get(
    client: httpx.AsyncClient,
    url: str,
    **kwargs,
) -> httpx.Response | None:
    max_retries = 3
    for attempt in range(max_retries):
        try:
            resp = await client.get(url, **kwargs)
        except (httpx.ConnectTimeout, httpx.ReadTimeout, httpx.ConnectError) as e:
            # Ğ¡ĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° â€” retry Ñ backoff
            wait = 2 ** attempt
            logger.warning(f"Network error on {url}: {e}. Retry in {wait}s (attempt {attempt+1}/{max_retries})")
            await asyncio.sleep(wait)
            continue

        if resp.status_code == 200:
            return resp

        if resp.status_code == 403:
            remaining = int(resp.headers.get("X-RateLimit-Remaining", "1"))
            if remaining == 0:
                reset_ts = int(resp.headers.get("X-RateLimit-Reset", "0"))
                import time as _time
                wait = max(1, reset_ts - int(_time.time())) + 1
                logger.warning(f"GitHub rate limit hit. Sleeping {wait}s until reset.")
                await asyncio.sleep(min(wait, 300))
                continue
            else:
                logger.warning(f"GitHub 403 (not rate limit): {url}")
                return None

        if resp.status_code >= 500:
            wait = 2 ** attempt
            logger.warning(f"GitHub 5xx ({resp.status_code}), retry in {wait}s")
            await asyncio.sleep(wait)
            continue

        return None

    return None
```

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #14 â€” Fallback Endpoints Ğ´Ğ»Ñ Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#14. `httpbin.org` ĞšĞ°Ğº Ğ•Ğ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Endpoint Ğ´Ğ»Ñ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸]]

  

**Ğ£Ğ¶Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¾** Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ #2 â€” ÑĞ¿Ğ¸ÑĞ¾Ğº `CHECK_URLS` ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 3 endpoint-Ğ°, Ğ¸ `_check_http` Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ Ğ½Ğ¸Ğ¼. Ğ•ÑĞ»Ğ¸ `httpbin.org` Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸Ğ´Ñ‘Ñ‚ Ñ‡ĞµÑ€ĞµĞ· `ifconfig.me` Ğ¸ `api.ipify.org`.

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #7 â€” Retry-Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ’ÑĞµÑ… Ğ¡ĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#7. ĞĞµÑ‚ Retry-Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ ĞĞ¸Ğ³Ğ´Ğµ]]

  

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:**

- `app/collectors/url_list.py` â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ retry

- `app/geo.py` â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ retry

  

#### Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ retry-Ğ´ĞµĞºĞ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€

  

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ» `app/retry.py`:

  

```python

# app/retry.py â€” ĞĞĞ’Ğ«Ğ™ Ğ¤ĞĞ™Ğ›

import asyncio

import logging

from functools import wraps

  

logger = logging.getLogger(__name__)

  
  

async def retry_async(

Â  Â  coro_func,

Â  Â  *args,

Â  Â  max_attempts: int = 3,

Â  Â  base_delay: float = 1.0,

Â  Â  **kwargs,

):

Â  Â  """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ async-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ retry Ğ¸ exponential backoff."""

Â  Â  for attempt in range(max_attempts):

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  return await coro_func(*args, **kwargs)

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  if attempt == max_attempts - 1:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"All {max_attempts} attempts failed for {coro_func.__name__}: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  raise

Â  Â  Â  Â  Â  Â  delay = base_delay * (2 ** attempt)

Â  Â  Â  Â  Â  Â  logger.debug(f"Attempt {attempt+1} failed, retrying in {delay}s: {e}")

Â  Â  Â  Â  Â  Â  await asyncio.sleep(delay)

```

  

#### ĞŸÑ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ² `url_list.py`:

  

```python

# app/collectors/url_list.py â€” Ğ¡Ğ¢ĞĞ›Ğ:

import logging

import httpx

from app.collectors.base import Collector

from app.retry import retry_async

  

logger = logging.getLogger(__name__)

  

class URLListCollector(Collector):

Â  Â  def __init__(self, urls: list[str]) -> None:

Â  Â  Â  Â  self.urls = urls

  

Â  Â  async def collect(self) -> list[tuple[str, str]]:

Â  Â  Â  Â  out: list[tuple[str, str]] = []

Â  Â  Â  Â  if not self.urls:

Â  Â  Â  Â  Â  Â  return out

Â  Â  Â  Â  async with httpx.AsyncClient(timeout=10) as client:

Â  Â  Â  Â  Â  Â  for url in self.urls:

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r = await retry_async(client.get, url, max_attempts=3)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if r.status_code == 200 and r.text:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  out.append((url, r.text))

Â  Â  Â  Â  Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"Failed to fetch {url} after retries")

Â  Â  Â  Â  return out

```

  

#### ĞŸÑ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ² `geo.py`:

  

```python

# app/geo.py â€” Ğ¡Ğ¢ĞĞ›Ğ:

import logging

import httpx

  

logger = logging.getLogger(__name__)

  

# ĞšÑÑˆ IP -> ÑÑ‚Ñ€Ğ°Ğ½Ğ° (Ğ¶Ğ¸Ğ²Ñ‘Ñ‚ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°)

_geo_cache: dict[str, str | None] = {}

  

async def country_by_ip(ip: str) -> str | None:

Â  Â  if ip in _geo_cache:

Â  Â  Â  Â  return _geo_cache[ip]

  

Â  Â  url = f"https://ipapi.co/{ip}/country/"

Â  Â  for attempt in range(2):

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  async with httpx.AsyncClient(timeout=3) as client:

Â  Â  Â  Â  Â  Â  Â  Â  resp = await client.get(url)

Â  Â  Â  Â  Â  Â  Â  Â  if resp.status_code == 429: Â # Rate limited

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning("ipapi.co rate limit hit")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  _geo_cache[ip] = None

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â  Â  Â  if resp.status_code != 200:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  cc = resp.text.strip().upper()

Â  Â  Â  Â  Â  Â  Â  Â  result = cc if len(cc) == 2 else None

Â  Â  Â  Â  Â  Â  Â  Â  _geo_cache[ip] = result

Â  Â  Â  Â  Â  Â  Â  Â  return result

Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  continue

  

Â  Â  _geo_cache[ip] = None

Â  Â  return None

```

  

**ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ñ‚Ğ°Ğº:** Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ in-memory ĞºÑÑˆ (`_geo_cache`) â€” Ğ¾Ğ´Ğ¸Ğ½ IP Ğ½Ğ¸ĞºĞ¾Ğ³Ğ´Ğ° Ğ½Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²Ğ°Ğ¶Ğ´Ñ‹. Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ #4 (Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ API), Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸.

  

---

  

## Ğ¤Ğ°Ğ·Ğ° 3: ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ

  

> Ğ¦ĞµĞ»ÑŒ: ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°.

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #3 â€” ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¡Ğ±Ğ¾Ñ€ Ğ”Ğ°Ğ½Ğ½Ñ‹Ñ…

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#3. Ğ’ÑĞµ Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ â€” ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞŸĞ¾Ñ‚ĞµÑ€Ñ Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸]]

  

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:**

- `app/collectors/github.py` â€” Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Semaphore

- `app/collectors/url_list.py` â€” Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹

- `app/pipeline.py` â€” Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²

- `app/pipeline.py` â€” Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ¾

  

#### Ğ’ GitHubCollector â€” Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· Semaphore:

  

> [!IMPORTANT]

> GitHub API Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ğµ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ñ‹, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ½ĞµĞ»ÑŒĞ·Ñ Ğ±ĞµĞ·Ğ´ÑƒĞ¼Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ `asyncio.Semaphore` Ğ´Ğ»Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (3-5 Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ…), ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ rate limit Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ.

  

```python

# app/collectors/github.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²:

import asyncio

  

# Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº GitHub API

_GITHUB_SEMAPHORE = asyncio.Semaphore(3)

  

async def _search_keyword(

    client: httpx.AsyncClient,

    keyword: str,

    search_type: str,  # "code" Ğ¸Ğ»Ğ¸ "repositories"

    pages: int,

) -> list[dict]:

    """ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¼Ñƒ ÑĞ»Ğ¾Ğ²Ñƒ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Semaphore."""

    results = []

    async with _GITHUB_SEMAPHORE:

        for page in range(1, pages + 1):

            endpoint = f"https://api.github.com/search/{search_type}"

            params = {"q": keyword, "per_page": 30, "page": page}

            resp = await _rate_limited_get(client, endpoint, params=params)

            if resp is None:

                break

            items = resp.json().get("items", [])

            if not items:

                break

            results.extend(items)

    return results

  

async def collect(self) -> list[tuple[str, str]]:

    """ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼."""

    async with httpx.AsyncClient(

        headers=self._headers(), timeout=15

    ) as client:

        # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ keywords Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾

        code_tasks = [

            _search_keyword(client, kw, "code", pages=5)

            for kw in self.keywords

        ]

        repo_tasks = [

            _search_keyword(client, kw, "repositories", pages=5)

            for kw in self.keywords

        ]

        all_results = await asyncio.gather(

            *code_tasks, *repo_tasks,

            return_exceptions=True,

        )

  

        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸

        code_items = []

        repo_items = []

        for i, result in enumerate(all_results):

            if isinstance(result, Exception):

                logger.warning(f"Search task {i} failed: {result}")

                continue

            if i < len(code_tasks):

                code_items.extend(result)

            else:

                repo_items.extend(result)

  

        # Ğ”Ğ°Ğ»ĞµĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° code_items Ğ¸ repo_items

        # (Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ‚Ğ¾Ğ¶Ğµ Ñ‡ĞµÑ€ĞµĞ· Semaphore â€” ÑĞ¼. Ğ´Ğ°Ğ»ĞµĞµ)

        out = []

        file_tasks = []

        for item in code_items:

            url = item.get("download_url") or item.get("html_url")

            if url:

                file_tasks.append(_fetch_file(client, url))

        file_results = await asyncio.gather(*file_tasks, return_exceptions=True)

        for r in file_results:

            if isinstance(r, tuple):

                out.append(r)

        return out

  

async def _fetch_file(

    client: httpx.AsyncClient, url: str

) -> tuple[str, str] | None:

    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Semaphore."""

    async with _GITHUB_SEMAPHORE:

        resp = await _rate_limited_get(client, url)

        if resp and resp.text:

            return (url, resp.text)

    return None

```

  

**Ğ­Ñ„Ñ„ĞµĞºÑ‚:** Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° 5 keywords Ã— 5 pages Ã— 2 types = 50 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (10+ ÑĞµĞºÑƒĞ½Ğ´), Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸Ğ´ÑƒÑ‚ Ğ¿Ğ¾ 3 Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¾ ~3-4 ÑĞµĞºÑƒĞ½Ğ´, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ rate-limit Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ.

  

#### Ğ’ URLListCollector â€” Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°:

  

```python

# app/collectors/url_list.py â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ collect():

async def collect(self) -> list[tuple[str, str]]:

Â  Â  if not self.urls:

Â  Â  Â  Â  return []

  

Â  Â  async def _fetch_one(client: httpx.AsyncClient, url: str) -> tuple[str, str] | None:

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  r = await client.get(url)

Â  Â  Â  Â  Â  Â  if r.status_code == 200 and r.text:

Â  Â  Â  Â  Â  Â  Â  Â  return (url, r.text)

Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  logger.warning(f"Failed to fetch {url}")

Â  Â  Â  Â  return None

  

Â  Â  out: list[tuple[str, str]] = []

Â  Â  async with httpx.AsyncClient(timeout=10) as client:

Â  Â  Â  Â  results = await asyncio.gather(*(_fetch_one(client, u) for u in self.urls))

Â  Â  Â  Â  out = [r for r in results if r is not None]

Â  Â  return out

```

  

#### Ğ’ Pipeline â€” Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¸ Ğ³ĞµĞ¾-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹:

  

```python

# app/pipeline.py â€” run_once():

# Ğ‘Ğ«Ğ›Ğ:

# Â  Â  for collector in collectors:

# Â  Â  Â  Â  raws.extend(await collector.collect())

# Ğ¡Ğ¢ĞĞ›Ğ:

Â  Â  results = await asyncio.gather(*(c.collect() for c in collectors))

Â  Â  raws: list[tuple[str, str]] = []

Â  Â  for result in results:

Â  Â  Â  Â  raws.extend(result)

  

# Ğ‘Ğ«Ğ›Ğ (Ğ³ĞµĞ¾-Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ):

# Â  Â  for item in validated:

# Â  Â  Â  Â  country = await country_by_ip(item.candidate.host)

# Ğ¡Ğ¢ĞĞ›Ğ:

Â  Â  # ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… IP Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾

Â  Â  unique_ips = {item.candidate.host for item in validated if _is_ip(item.candidate.host)}

Â  Â  geo_tasks = {ip: country_by_ip(ip) for ip in unique_ips}

Â  Â  geo_results = await asyncio.gather(*geo_tasks.values())

Â  Â  ip_to_country = dict(zip(geo_tasks.keys(), geo_results))

  

Â  Â  for item in validated:

Â  Â  Â  Â  country = ip_to_country.get(item.candidate.host) if _is_ip(item.candidate.host) else None

Â  Â  Â  Â  # ... Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ ...

```

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #4 â€” Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ GeoIP Ğ’Ğ¼ĞµÑÑ‚Ğ¾ API

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#4. Geo-API ĞĞ´Ğ¸Ğ½ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° IP â€” ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ‘ÑƒÑ‚Ñ‹Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ“Ğ¾Ñ€Ğ»Ğ¾]]

  

**Ğ˜Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ** (Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ipapi.co):

  

```txt

# requirements.txt â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ:

geoip2==4.8.1

```

  

```python

# app/geo.py â€” Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ğ°:

import geoip2.database

import logging

  

logger = logging.getLogger(__name__)

  

_reader = None

  

def _get_reader():

Â  Â  global _reader

Â  Â  if _reader is None:

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  _reader = geoip2.database.Reader("GeoLite2-Country.mmdb")

Â  Â  Â  Â  except Exception:

Â  Â  Â  Â  Â  Â  logger.error("GeoLite2-Country.mmdb not found. Download from MaxMind.")

Â  Â  return _reader

  

async def country_by_ip(ip: str) -> str | None:

Â  Â  reader = _get_reader()

Â  Â  if reader is None:

Â  Â  Â  Â  return None

Â  Â  try:

Â  Â  Â  Â  response = reader.country(ip)

Â  Â  Â  Â  return response.country.iso_code

Â  Â  except Exception:

Â  Â  Â  Â  return None

```

  

**ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ñ‚Ğ°Ğº:** Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° â€” **Ğ½ÑƒĞ»ĞµĞ²Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ°**, **Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğµ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ñ‹**, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½. Ğ¤Ğ°Ğ¹Ğ» `GeoLite2-Country.mmdb` (âˆ¼6MB) ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾ Ñ ÑĞ°Ğ¹Ñ‚Ğ° MaxMind.

  

> [!TIP]

> ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ `ipapi.co` ĞºĞ°Ğº fallback, Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº.

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #5 â€” Batch-Ğ—Ğ°Ğ¿Ğ¸ÑÑŒ Ğ² Ğ‘Ğ”

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#5. SQLite + Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞĞ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ â€” ĞŸĞ»Ğ¾Ñ…Ğ¾ ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ]]

  

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:**

- `app/storage.py` â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ batch-Ğ¼ĞµÑ‚Ğ¾Ğ´

- `app/pipeline.py` â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ batch Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ°

  

```python

# app/storage.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ Ğ¼ĞµÑ‚Ğ¾Ğ´:

def batch_upsert_proxies(self, items: list[dict]) -> int:

Â  Â  """ĞœĞ°ÑÑĞ¾Ğ²Ñ‹Ğ¹ upsert. items = ÑĞ¿Ğ¸ÑĞ¾Ğº ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ Ñ ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸:

Â  Â  proxy_type, host, port, source, country, is_alive, latency_ms

Â  Â  """

Â  Â  saved = 0

Â  Â  with Session(self.engine) as session:

Â  Â  Â  Â  for item in items:

Â  Â  Â  Â  Â  Â  existing = session.scalar(

Â  Â  Â  Â  Â  Â  Â  Â  select(Proxy).where(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Proxy.proxy_type == item["proxy_type"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Proxy.host == item["host"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Proxy.port == item["port"],

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  if not existing:

Â  Â  Â  Â  Â  Â  Â  Â  existing = Proxy(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  proxy_type=item["proxy_type"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  host=item["host"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  port=item["port"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source=item["source"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  country=item["country"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_alive=item["is_alive"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  latency_ms=item["latency_ms"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  success_rate=1.0 if item["is_alive"] else 0.0,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score=self._score(item["is_alive"], item["latency_ms"], 1.0 if item["is_alive"] else 0.0),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  last_checked_at=datetime.utcnow(),

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  session.add(existing)

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  old_sr = existing.success_rate

Â  Â  Â  Â  Â  Â  Â  Â  new_sr = (old_sr * 0.8) + (1.0 if item["is_alive"] else 0.0) * 0.2

Â  Â  Â  Â  Â  Â  Â  Â  existing.is_alive = item["is_alive"]

Â  Â  Â  Â  Â  Â  Â  Â  existing.latency_ms = item["latency_ms"]

Â  Â  Â  Â  Â  Â  Â  Â  existing.country = item["country"] or existing.country

Â  Â  Â  Â  Â  Â  Â  Â  existing.source = item["source"]

Â  Â  Â  Â  Â  Â  Â  Â  existing.success_rate = new_sr

Â  Â  Â  Â  Â  Â  Â  Â  existing.score = self._score(item["is_alive"], item["latency_ms"], new_sr)

Â  Â  Â  Â  Â  Â  Â  Â  existing.last_checked_at = datetime.utcnow()

  

Â  Â  Â  Â  Â  Â  session.add(Observation(

Â  Â  Â  Â  Â  Â  Â  Â  proxy_type=item["proxy_type"],

Â  Â  Â  Â  Â  Â  Â  Â  host=item["host"],

Â  Â  Â  Â  Â  Â  Â  Â  port=item["port"],

Â  Â  Â  Â  Â  Â  Â  Â  is_alive=item["is_alive"],

Â  Â  Â  Â  Â  Â  Â  Â  latency_ms=item["latency_ms"],

Â  Â  Â  Â  Â  Â  Â  Â  source=item["source"],

Â  Â  Â  Â  Â  Â  ))

Â  Â  Â  Â  Â  Â  saved += 1

  

Â  Â  Â  Â  session.commit() Â # ĞĞ”Ğ˜Ğ ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚ Ğ½Ğ° Ğ²ÑÑ Ğ¿Ğ°Ñ‡ĞºÑƒ

Â  Â  return saved

```

  

**Ğ­Ñ„Ñ„ĞµĞºÑ‚:** Ğ’Ğ¼ĞµÑÑ‚Ğ¾ 500 Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ğ¹ â€” Ğ¾Ğ´Ğ½Ğ°. ĞĞ° SQLite ÑÑ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² **10-50 Ñ€Ğ°Ğ·** Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸.

  

---

  

## Ğ¤Ğ°Ğ·Ğ° 4: ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞšĞ¾Ğ´Ğ°

  

> Ğ¦ĞµĞ»ÑŒ: ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ‚Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹.

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #6 â€” Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#6. ĞĞµÑ‚ Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ â€” Ğ’Ğ¾Ğ¾Ğ±Ñ‰Ğµ ĞĞ¸ĞºĞ°ĞºĞ¾Ğ³Ğ¾]]

  

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:**

- `app/main.py` â€” Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºĞ¾Ñ€Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ logger

- Ğ’ÑĞµ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ â€” Ğ·Ğ°Ğ¼ĞµĞ½Ğ° `print()` Ğ½Ğ° `logger.*`

  

#### Ğ¨Ğ°Ğ³ 1: ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ² `main.py`

  

```python

# app/main.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ main():

import logging

  

logging.basicConfig(

Â  Â  level=logging.INFO,

Â  Â  format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",

Â  Â  datefmt="%Y-%m-%d %H:%M:%S",

)

```

  

#### Ğ¨Ğ°Ğ³ 2: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸

  

| Ğ¤Ğ°Ğ¹Ğ» | Ğ§Ñ‚Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ |

|------|---------------|

| `pipeline.py` | ĞĞ°Ñ‡Ğ°Ğ»Ğ¾/ĞºĞ¾Ğ½ĞµÑ† Ñ†Ğ¸ĞºĞ»Ğ°, ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ raw/candidates/alive |

| `github.py` | Rate limit warnings, ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²/repos |

| `validator.py` | ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ (X Ğ¸Ğ· Y Ğ¶Ğ¸Ğ²Ñ‹Ñ…) |

| `geo.py` | Rate limit Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ API |

| `scheduler.py` | Ğ—Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ `print()` Ğ½Ğ° `logger.info()` |

  

#### Ğ¨Ğ°Ğ³ 3: ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ĞºĞ¾Ğ´Ğ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ°

  

```python

# app/pipeline.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ¸ Ğ² run_once():

import logging

logger = logging.getLogger(__name__)

  

async def run_once(self) -> dict:
    logger.info("=== Pipeline cycle started ===")
    # ... ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ...
    logger.info(f"Collected {len(raws)} raw texts from {len(collectors)} collectors")
    # ... Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ...
    logger.info(f"Normalized into {len(candidates)} proxy candidates")
    # ... Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ ...
    alive_count = sum(1 for v in validated if v.is_alive)
    logger.info(f"Validation complete: {alive_count}/{len(candidates)} alive")
    # ... Ğ³ĞµĞ¾ ...
    logger.info(f"Geo resolved for {len(unique_ips)} unique IPs")
    # ... Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ ...
    logger.info(f"Saved {saved} proxies to database")
    logger.info(f"=== Pipeline cycle finished in {elapsed:.1f}s ===")
    return {"raw": len(raws), "candidates": len(candidates), "alive": alive_count, "saved": saved}

```

  

```python

# app/collectors/github.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²:

import logging

logger = logging.getLogger(__name__)

  

# Ğ’ collect():
    logger.info(f"GitHub search: {len(self.keywords)} keywords, searching...")
    # ... Ğ¿Ğ¾ÑĞ»Ğµ ÑĞ±Ğ¾Ñ€Ğ° ...
    logger.info(f"GitHub: found {len(code_items)} code results, {len(repo_items)} repos")
    # ... Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² ...
    logger.info(f"GitHub: downloaded {len(out)} files with proxy data")

```

  

```python

# app/validator.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ:

import logging

logger = logging.getLogger(__name__)

  

# Ğ’ validate_many(), Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ:
    alive = sum(1 for r in results if r.is_alive)
    dead = len(results) - alive
    avg_latency = sum(r.latency_ms for r in results if r.latency_ms) / max(alive, 1)
    logger.info(
        f"Validation: {alive} alive, {dead} dead out of {len(results)} "
        f"(avg latency: {avg_latency:.0f}ms)"
    )

```

  

```python

# app/scheduler.py â€” Ğ—ĞĞœĞ•ĞĞ˜Ğ¢Ğ¬ print() Ğ½Ğ° logger:

import logging

logger = logging.getLogger(__name__)

  

# Ğ‘Ğ«Ğ›Ğ:
#     print(f"Starting scheduled run...")
# Ğ¡Ğ¢ĞĞ›Ğ:
    logger.info(f"Scheduler: starting pipeline run (interval: {interval_min}m)")

```

  

> [!TIP]

> Ğ”Ğ»Ñ production-ÑÑ€ĞµĞ´Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ **Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¾Ğ²** Ñ‡ĞµÑ€ĞµĞ· `logging.handlers.RotatingFileHandler` Ğ¸ Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ **JSON-Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚** Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°:

  

```python

# app/main.py â€” ĞĞŸĞ¦Ğ˜ĞĞĞĞ›Ğ¬ĞĞĞ¯ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° (Ğ²Ğ¼ĞµÑÑ‚Ğ¾ basicConfig):

import logging

from logging.handlers import RotatingFileHandler

  

def setup_logging(log_file: str = "parser.log", level: str = "INFO"):
    root = logging.getLogger()
    root.setLevel(getattr(logging, level.upper()))
  
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸
    console = logging.StreamHandler()
    console.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%H:%M:%S",
    ))
    root.addHandler(console)
  
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ»Ğ° (Ñ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¼Ğ°ĞºÑ 5MB Ã— 3 Ñ„Ğ°Ğ¹Ğ»Ğ°)
    file_handler = RotatingFileHandler(
        log_file, maxBytes=5_000_000, backupCount=3, encoding="utf-8"
    )
    file_handler.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    ))
    root.addHandler(file_handler)

```

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #10 â€” Ğ—Ğ°Ğ¼ĞµĞ½Ğ° `datetime.utcnow()`

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#10. `datetime.utcnow()` â€” Deprecated Ğ¸ ĞĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹]]

  

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:**

- `app/storage.py`

- `app/models.py`

  

Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ğ°:

```python

# Ğ‘Ğ«Ğ›Ğ:

datetime.utcnow()

# Ğ¡Ğ¢ĞĞ›Ğ:

from datetime import datetime, timezone

datetime.now(timezone.utc)

```

  

Ğ’ `models.py` â€” Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ `default=datetime.utcnow` Ğ½Ğ° `default=lambda: datetime.now(timezone.utc)`.

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #12 â€” Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¢ĞµÑÑ‚Ğ¾Ğ²

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#12. Ğ¢ĞµÑÑ‚Ñ‹ â€” ĞŸĞ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ]]

  

**ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:**

  

| Ğ¢ĞµÑÑ‚ | Ğ¤Ğ°Ğ¹Ğ» | Ğ§Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ |

|------|------|---------------|

| `test_normalizer_extended.py` | `normalizer.py` | SS URI, VMess URI, JSON format, user:pass, space/tab, Ğ´ÑƒĞ±Ğ»Ğ¸, Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚ |

| `test_validator_routing.py` | `validator.py` | ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ° |

| `test_score.py` | `storage.py` | Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ° ÑĞºĞ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ edge cases |

| `test_pipeline_stats.py` | `pipeline.py` | ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ counts (mock collectors) |

| `test_geo_cache.py` | `geo.py` | ĞšÑÑˆ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚, Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ½Ğµ Ğ´ĞµĞ»Ğ°ÑÑ‚ HTTP |

  

#### ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²

  

```python

# tests/test_normalizer_extended.py â€” ĞĞĞ’Ğ«Ğ™ Ğ¤ĞĞ™Ğ›

import pytest

from app.normalizer import parse_candidates

  

class TestStandardFormat:

    def test_http_proxy(self):

        text = "http://1.2.3.4:8080"

        result = parse_candidates(text, "test")

        assert len(result) == 1

        assert result[0].proxy_type == "http"

        assert result[0].host == "1.2.3.4"

        assert result[0].port == 8080

  

    def test_socks5_with_auth(self):

        text = "socks5://user:password@1.2.3.4:1080"

        result = parse_candidates(text, "test")

        assert len(result) == 1

        assert result[0].proxy_type == "socks5"

        assert result[0].host == "1.2.3.4"

  

    def test_deduplication(self):

        text = "http://1.2.3.4:8080\nhttp://1.2.3.4:8080"

        result = parse_candidates(text, "test")

        assert len(result) == 1

  

class TestShadowsocksURI:

    def test_ss_base64(self):

        import base64

        payload = base64.b64encode(b"aes-256-cfb:password@5.6.7.8:443").decode()

        text = f"ss://{payload}"

        result = parse_candidates(text, "test")

        assert any(r.proxy_type == "ss" and r.host == "5.6.7.8" and r.port == 443 for r in result)

  

class TestVMessURI:

    def test_vmess_base64(self):

        import base64, json

        config = json.dumps({"add": "9.8.7.6", "port": 443, "id": "xxx"})

        payload = base64.b64encode(config.encode()).decode()

        text = f"vmess://{payload}"

        result = parse_candidates(text, "test")

        assert any(r.proxy_type == "vmess" and r.host == "9.8.7.6" and r.port == 443 for r in result)

  

class TestSpaceTabFormat:

    def test_space_separated(self):

        text = "1.2.3.4 8080"

        result = parse_candidates(text, "test")

        assert len(result) >= 1

        assert any(r.host == "1.2.3.4" and r.port == 8080 for r in result)

  

    def test_tab_separated(self):

        text = "1.2.3.4\t8080"

        result = parse_candidates(text, "test")

        assert len(result) >= 1

        assert any(r.host == "1.2.3.4" and r.port == 8080 for r in result)

  

class TestJSONFormat:

    def test_json_server_port(self):

        text = '{"server": "10.0.0.1", "server_port": 1080}'

        result = parse_candidates(text, "test")

        assert any(r.host == "10.0.0.1" and r.port == 1080 for r in result)

  

class TestEdgeCases:

    def test_empty_text(self):

        result = parse_candidates("", "test")

        assert result == []

  

    def test_garbage_text(self):

        result = parse_candidates("hello world no proxies here!", "test")

        assert result == []

```

  

```python

# tests/test_validator_routing.py â€” ĞĞĞ’Ğ«Ğ™ Ğ¤ĞĞ™Ğ›

import pytest

import asyncio

from unittest.mock import AsyncMock, patch

from app.validator import _check

from app.normalizer import ProxyCandidate

  

class TestValidatorRouting:

    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ°."""

  

    @pytest.mark.asyncio

    @patch("app.validator._check_http")

    async def test_http_uses_http_strategy(self, mock_http):

        c = ProxyCandidate(proxy_type="http", host="1.2.3.4", port=8080, source="test")

        await _check(c, timeout_sec=5.0)

        mock_http.assert_called_once_with(c, 5.0)

  

    @pytest.mark.asyncio

    @patch("app.validator._check_socks")

    async def test_socks5_uses_socks_strategy(self, mock_socks):

        c = ProxyCandidate(proxy_type="socks5", host="1.2.3.4", port=1080, source="test")

        await _check(c, timeout_sec=5.0)

        mock_socks.assert_called_once_with(c, 5.0)

  

    @pytest.mark.asyncio

    @patch("app.validator._check_tcp_only")

    async def test_mtproto_uses_tcp_strategy(self, mock_tcp):

        c = ProxyCandidate(proxy_type="mtproto", host="1.2.3.4", port=443, source="test")

        await _check(c, timeout_sec=5.0)

        mock_tcp.assert_called_once_with(c, 5.0)

  

    @pytest.mark.asyncio

    @patch("app.validator._check_tcp_only")

    async def test_ss_uses_tcp_strategy(self, mock_tcp):

        c = ProxyCandidate(proxy_type="ss", host="1.2.3.4", port=443, source="test")

        await _check(c, timeout_sec=5.0)

        mock_tcp.assert_called_once_with(c, 5.0)

```

  

```python

# tests/test_geo_cache.py â€” ĞĞĞ’Ğ«Ğ™ Ğ¤ĞĞ™Ğ›

import pytest

from unittest.mock import patch, AsyncMock

  

class TestGeoCache:

    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑÑˆ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ½Ğµ Ğ´ĞµĞ»Ğ°ÑÑ‚ HTTP."""

  

    @pytest.mark.asyncio

    async def test_cache_prevents_duplicate_requests(self):

        # Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ API-Ğ²ĞµÑ€ÑĞ¸Ñ geo.py (Ğ´Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ½Ğ° MaxMind)

        from app import geo

        geo._geo_cache.clear()

  

        with patch("httpx.AsyncClient.get", new_callable=AsyncMock) as mock_get:

            mock_get.return_value.status_code = 200

            mock_get.return_value.text = "US"

  

            result1 = await geo.country_by_ip("1.2.3.4")

            result2 = await geo.country_by_ip("1.2.3.4")  # Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ²Ğ·ÑÑ‚ÑŒÑÑ Ğ¸Ğ· ĞºÑÑˆĞ°

  

            assert result1 == "US"

            assert result2 == "US"

            assert mock_get.call_count == 1  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ HTTP-Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ!

  

        geo._geo_cache.clear()

```

  

---

  

## Ğ¤Ğ°Ğ·Ğ° 5: ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ

  

> Ğ¦ĞµĞ»ÑŒ: Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #9 â€” TTL Ğ´Ğ»Ñ Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ²

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#9. ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¡ĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ—Ğ°Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ ĞĞ°Ğ²ÑĞµĞ³Ğ´Ğ°]]

  

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:**

- `app/config.py` â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ `REPO_RESCAN_HOURS`

- `app/storage.py` â€” Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ `enqueue_repo()`

  

```python

# app/config.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬:

repo_rescan_hours: int = int(os.getenv("REPO_RESCAN_HOURS", "48"))

```

  

```python

# app/storage.py â€” Ğ˜Ğ—ĞœĞ•ĞĞ˜Ğ¢Ğ¬ enqueue_repo():

def enqueue_repo(self, repo_full_name: str, note: str | None = None, force: bool = False) -> tuple[bool, str]:

Â  Â  repo = repo_full_name.strip().lower()

Â  Â  with Session(self.engine) as session:

Â  Â  Â  Â  existing = session.scalar(select(RepoTask).where(RepoTask.repo_full_name == repo))

Â  Â  Â  Â  if existing:

Â  Â  Â  Â  Â  Â  if existing.status == "done":

Â  Â  Â  Â  Â  Â  Â  Â  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ TTL

Â  Â  Â  Â  Â  Â  Â  Â  from app.config import settings

Â  Â  Â  Â  Â  Â  Â  Â  if existing.last_analyzed_at and not force:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  age_hours = (datetime.now(timezone.utc) - existing.last_analyzed_at).total_seconds() / 3600

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if age_hours < settings.repo_rescan_hours:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return False, "already_analyzed"

Â  Â  Â  Â  Â  Â  Â  Â  # TTL Ğ¸ÑÑ‚Ñ‘Ğº â€” Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ

Â  Â  Â  Â  Â  Â  Â  Â  existing.status = "pending"

Â  Â  Â  Â  Â  Â  Â  Â  existing.note = note or "rescan_ttl"

Â  Â  Â  Â  Â  Â  Â  Â  session.commit()

Â  Â  Â  Â  Â  Â  Â  Â  return True, "requeued"

Â  Â  Â  Â  Â  Â  return False, "already_queued"

Â  Â  Â  Â  session.add(RepoTask(repo_full_name=repo, status="pending", note=note))

Â  Â  Â  Â  session.commit()

Â  Â  Â  Â  return True, "queued"

```

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #13 â€” ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¸ Ğ Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ ĞœÑ‘Ñ€Ñ‚Ğ²Ñ‹Ñ… ĞŸÑ€Ğ¾ĞºÑĞ¸

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#13. ĞĞµÑ‚ ĞÑ‡Ğ¸ÑÑ‚ĞºĞ¸ ĞœÑ‘Ñ€Ñ‚Ğ²Ñ‹Ñ… ĞŸÑ€Ğ¾ĞºÑĞ¸]]

  

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:**

- `app/config.py` â€” Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ TTL

- `app/storage.py` â€” Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸

- `app/pipeline.py` â€” Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ°

  

```python

# app/config.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬:

proxy_dead_ttl_days: int = int(os.getenv("PROXY_DEAD_TTL_DAYS", "7"))

observation_ttl_days: int = int(os.getenv("OBSERVATION_TTL_DAYS", "30"))

```

  

```python

# app/storage.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹:

def cleanup_dead_proxies(self, dead_ttl_days: int) -> int:

Â  Â  """Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ĞºÑĞ¸, Ğ¼Ñ‘Ñ€Ñ‚Ğ²Ñ‹Ğµ Ğ±Ğ¾Ğ»ĞµĞµ N Ğ´Ğ½ĞµĞ¹."""

Â  Â  cutoff = datetime.now(timezone.utc) - timedelta(days=dead_ttl_days)

Â  Â  with Session(self.engine) as session:

Â  Â  Â  Â  count = session.query(Proxy).filter(

Â  Â  Â  Â  Â  Â  Proxy.is_alive.is_(False),

Â  Â  Â  Â  Â  Â  Proxy.last_checked_at < cutoff,

Â  Â  Â  Â  ).delete()

Â  Â  Â  Â  session.commit()

Â  Â  Â  Â  return count

  

def cleanup_old_observations(self, ttl_days: int) -> int:

Â  Â  """Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ observation Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ ÑÑ‚Ğ°Ñ€ÑˆĞµ N Ğ´Ğ½ĞµĞ¹."""

Â  Â  cutoff = datetime.now(timezone.utc) - timedelta(days=ttl_days)

Â  Â  with Session(self.engine) as session:

Â  Â  Â  Â  count = session.query(Observation).filter(

Â  Â  Â  Â  Â  Â  Observation.checked_at < cutoff,

Â  Â  Â  Â  ).delete()

Â  Â  Â  Â  session.commit()

Â  Â  Â  Â  return count

```

  

```python

# app/pipeline.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ Ğ² ĞºĞ¾Ğ½ĞµÑ† run_once():

Â  Â  # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ°

Â  Â  dead_removed = self.storage.cleanup_dead_proxies(self.settings.proxy_dead_ttl_days)

Â  Â  obs_removed = self.storage.cleanup_old_observations(self.settings.observation_ttl_days)

Â  Â  logger.info(f"Cleanup: {dead_removed} dead proxies, {obs_removed} old observations removed")

```

  

---

  

### ğŸ”§ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ #11 â€” Ğ¡Ğ²ÑĞ·ÑŒ Ğ‘Ğ¾Ñ‚Ğ° Ğ¸ ĞŸĞ°Ñ€ÑĞµÑ€Ğ°

  

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**: [[PARSER_ISSUES#11. Ğ‘Ğ¾Ñ‚ Ğ¸ ĞŸĞ°Ñ€ÑĞµÑ€ â€” Ğ”Ğ²Ğ° ĞÑ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞŸÑ€Ğ¾Ñ†ĞµÑÑĞ° Ğ‘ĞµĞ· Ğ¡Ğ²ÑĞ·Ğ¸]]

  

**Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´:** ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ñ‚Ğ° Ğ¸ Ğ¿Ğ°Ñ€ÑĞµÑ€ Ğ² Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. `python-telegram-bot` Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ `asyncio`, Ğ¸ `APScheduler` Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ÑŒ event loop Ğ±Ğ¾Ñ‚Ğ°.

  

```python

# app/main.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ:

sub.add_parser("all-in-one", help="Run bot + scheduler in one process")

  

# Ğ’ main():

elif args.cmd == "all-in-one":

Â  Â  run_all_in_one(settings)

```

  

```python

# app/bot.py â€” Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ¢Ğ¬ run_all_in_one():

async def run_all_in_one_job(context: ContextTypes.DEFAULT_TYPE) -> None:

Â  Â  """Job, Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¸Ğ· event loop Ğ±Ğ¾Ñ‚Ğ°."""

Â  Â  from app.pipeline import Pipeline

Â  Â  from app.config import settings

Â  Â  pipeline = Pipeline(settings)

Â  Â  stats = await pipeline.run_once()

Â  Â  logger.info(f"Pipeline run: {stats}")

  

def run_all_in_one(settings: Settings) -> None:

Â  Â  bot = AdminBot(settings)

Â  Â  app = Application.builder().token(settings.telegram_bot_token).build()

Â  Â  # ... Ğ²ÑĞµ handlers ĞºĞ°Ğº Ğ² run_bot() ...

Â  Â  # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ°Ñ€ÑĞµÑ€ ĞºĞ°Ğº repeating job

Â  Â  app.job_queue.run_repeating(

Â  Â  Â  Â  run_all_in_one_job,

Â  Â  Â  Â  interval=settings.schedule_minutes * 60,

Â  Â  Â  Â  first=15,

Â  Â  )

Â  Â  app.run_polling(close_loop=False)

```

  

**Ğ­Ñ„Ñ„ĞµĞºÑ‚:** ĞĞ´Ğ½Ğ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°, Ğ±Ğ¾Ñ‚ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‚Ñ€Ğ¸Ğ³ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ†Ğ¸ĞºĞ», Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ real-time ÑÑ‚Ğ°Ñ‚ÑƒÑ.

  

---

  

## ğŸ“Š Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ ĞÑ†ĞµĞ½ĞºĞ° Ğ¢Ñ€ÑƒĞ´Ğ¾Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚

  

| Ğ¤Ğ°Ğ·Ğ° | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ | ĞÑ†ĞµĞ½ĞºĞ° (Ñ‡Ğ°ÑÑ‹) | Ğ­Ñ„Ñ„ĞµĞºÑ‚ |

|------|----------|:------------:|--------|

| **Ğ¤Ğ°Ğ·Ğ° 1** | Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ + Normalizer | 3-4 | ğŸ”¥ +300% Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… |

| **Ğ¤Ğ°Ğ·Ğ° 2** | Rate limit + retry + fallback | 2-3 | ğŸ›¡ Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ±Ğ¾ÑĞ¼ |

| **Ğ¤Ğ°Ğ·Ğ° 3** | ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ + GeoIP + batch | 3-4 | âš¡ Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 5-10 Ñ€Ğ°Ğ· |

| **Ğ¤Ğ°Ğ·Ğ° 4** | Ğ›Ğ¾Ğ³Ğ¸ + datetime + Ñ‚ĞµÑÑ‚Ñ‹ | 2-3 | ğŸ” ĞÑ‚Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ |

| **Ğ¤Ğ°Ğ·Ğ° 5** | TTL + Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° + all-in-one | 2-3 | ğŸ— Ğ”Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ |

| **Ğ˜Ñ‚Ğ¾Ğ³Ğ¾** | | **12-17** | ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€ÑĞµÑ€ |

  

---

  

> **Ğ¡Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹:**

> - [[Ğ¡Ñ…ĞµĞ¼Ğ°]] â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹

> - [[ğŸš¨ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ”Ñ‹Ñ€Ñ‹ Ğ¸ ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞŸĞ°Ñ€ÑĞµÑ€Ğ°]] â€” Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼